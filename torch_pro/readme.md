# PyTorch 线性回归示例 (test4.py)

该项目是使用 PyTorch 实现线性回归的一个简单而完整的示例。代码演示了从数据生成到模型训练，再到结果可视化的全过程，并进行了优化和详细注释。

## 脚本核心流程

`test4.py` 的执行流程可以分为以下几个关键步骤：

### 1. 环境与设备准备

- **配置 `matplotlib`**:
  - 为了在图表中正确显示中文，脚本首先配置 `matplotlib` 使用中文字体（如 'SimHei'）。
  - 同时，设置 `axes.unicode_minus=False` 以确保负号可以正常显示。
- **自动选择计算设备**:
  - 脚本会自动检测环境中是否有可用的 CUDA GPU。
  - 如果检测到 GPU，则将计算设备设置为 `cuda`，否则回退到 `cpu`。
  - 这样可以充分利用硬件资源，加速训练过程。

### 2. 数据生成

- **设置随机种子**: `torch.manual_seed(42)` 用于固定随机数生成器的状态，确保每次运行代码时生成的“随机”数据都是一样的，这有利于结果的复现和调试。
- **创建特征和标签**:
  - 特征 `X` 是一个从 0 到 10 之间均匀分布的 100 个点。
  - 标签 `y` 是根据一个真实的线性关系 `y = 2.5 * X + 1.0` 生成的，并额外添加了高斯噪声 `torch.randn(...)`，以模拟真实世界中带有随机干扰的数据。
- **数据迁移**: 生成的 `X` 和 `y` 张量都被移动到第一步选择好的计算设备上（GPU 或 CPU）。

### 3. 模型定义

- **`LinearRegression` 类**:
  - 定义了一个继承自 `torch.nn.Module` 的模型类。这是所有 PyTorch 模型的基类。
  - 在构造函数 `__init__` 中，定义了一个 `nn.Linear(1, 1)` 层，表示该模型接受 1 个输入特征并产生 1 个输出特征。
  - 在 `forward` 方法中，定义了数据通过模型时的前向传播逻辑，即直接将输入 `x` 通过线性层。

### 4. 训练配置

- **实例化模型**: 创建 `LinearRegression` 类的实例，并将其移动到指定设备。
- **定义损失函数**: 选择均方误差损失 `nn.MSELoss()`，它用于衡量模型预测值与真实值之间的差距。
- **定义优化器**: 选择随机梯度下降 `torch.optim.SGD`，它负责根据损失函数计算出的梯度来更新模型的参数（权重和偏置），`lr=0.01` 是学习率。

### 5. 模型训练

- **训练循环**: 通过一个 `for` 循环迭代指定的 `epochs` 次数。
- **核心步骤**:
  1.  `model.train()`: 将模型设置为训练模式。
  2.  `optimizer.zero_grad()`: 清除上一轮迭代中累积的梯度。
  3.  `outputs = model(X)`: 将数据输入模型，进行前向传播，得到预测结果。
  4.  `loss = criterion(outputs, y)`: 计算预测结果与真实标签之间的损失。
  5.  `loss.backward()`: 进行反向传播，计算损失相对于模型参数的梯度。
  6.  `optimizer.step()`: 优化器根据梯度更新模型的参数。

### 6. 结果可视化

- **评估模式**: `model.eval()` 将模型切换到评估模式。这是一个好习惯，可以关闭 Dropout 等在评估时不需要的层。
- **无梯度计算**: 使用 `with torch.no_grad():` 上下文管理器，在该代码块内停止计算梯度，从而节省内存和计算资源。
- **数据转换与绘图**:
  - 使用训练好的模型对输入 `X` 进行预测。
  - 将所有需要绘图的张量（原始数据 `X`, `y` 和预测数据 `predicted`）从计算设备移回 CPU，并转换为 NumPy 数组。
  - 使用 `matplotlib.pyplot` 将原始数据绘制成散点图，并将模型的拟合结果绘制成一条直线。
  - 图表的标题会显示最终学习到的权重和偏置，并附有坐标轴标签和网格，以增强可读性。
