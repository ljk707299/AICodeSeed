# PyTorch 基础与进阶示例代码说明（demo_02）

本目录包含多个 PyTorch/Numpy 神经网络基础与进阶示例，涵盖了从最基础的 numpy 手写神经网络到 PyTorch 自动求导、模块化建模、优化器使用、动态网络等常见深度学习开发场景。

---

## PyTorch 训练流程简述

PyTorch 训练神经网络的一般标准流程如下：

1. **数据准备**
   - 构造输入数据（如张量、Dataset、DataLoader等），并根据需要迁移到GPU。
2. **模型定义**
   - 继承 `nn.Module` 或使用 `nn.Sequential` 定义网络结构。
3. **损失函数定义**
   - 选择合适的损失函数（如 `nn.MSELoss`、`nn.CrossEntropyLoss` 等）。
4. **优化器定义**
   - 选择优化器（如 `torch.optim.SGD`、`torch.optim.Adam`），并将模型参数传入。
5. **训练循环**
   - 通常包括多个 epoch，每个 epoch 包含：
     1. 前向传播：将输入数据传入模型，得到预测结果。
     2. 计算损失：用损失函数衡量预测与真实标签的差距。
     3. 梯度清零：`optimizer.zero_grad()` 清除上一步的梯度。
     4. 反向传播：`loss.backward()` 自动计算所有可学习参数的梯度。
     5. 参数更新：`optimizer.step()` 根据梯度更新参数。
   - 可选：打印损失、保存模型、调整学习率等。
6. **评估与推理**
   - 切换到 `model.eval()`，使用 `torch.no_grad()` 进行模型评估或推理，避免计算梯度以节省资源。

> **简要流程图**：
> 数据准备 → 模型定义 → 损失函数 → 优化器 → [前向传播 → 损失计算 → 梯度清零 → 反向传播 → 参数更新] × N → 评估/推理

---

## 各脚本功能与流程简介

### 1. test01.py —— Numpy手写两层神经网络
- **内容**：使用 Numpy 实现两层全连接神经网络（含 ReLU 激活），手动实现前向传播、反向传播和参数更新。
- **流程**：
  1. 随机生成输入/输出数据和权重参数
  2. 前向传播：线性->ReLU->线性
  3. 计算均方误差损失
  4. 手动反向传播计算梯度
  5. 梯度下降更新参数

### 2. test02.py —— PyTorch手写两层神经网络（无自动求导）
- **内容**：用 PyTorch 张量实现两层神经网络，手动实现前向传播和反向传播（不使用 autograd），支持 GPU。
- **流程**：
  1. 随机生成输入/输出数据和权重参数
  2. 前向传播：线性->ReLU->线性
  3. 计算均方误差损失
  4. 手动反向传播计算梯度
  5. 梯度下降更新参数

### 3. test03.py —— PyTorch自动求导两层神经网络
- **内容**：用 PyTorch 张量和 autograd 实现两层神经网络，自动反向传播，手动参数更新。
- **流程**：
  1. 随机生成输入/输出数据和权重参数（requires_grad=True）
  2. 前向传播：线性->ReLU->线性
  3. 计算均方误差损失
  4. autograd 自动反向传播
  5. 手动梯度下降更新参数并清零梯度

### 4. test04.py —— nn.Sequential模块化建模
- **内容**：用 nn.Sequential 定义三层（含ReLU）神经网络，使用 MSELoss，手动参数更新。
- **流程**：
  1. 随机生成输入/输出数据
  2. nn.Sequential 定义网络结构
  3. 前向传播、计算损失
  4. 反向传播
  5. 手动参数更新

### 5. test05.py —— 支持GPU的nn.Sequential建模
- **内容**：与 test04 类似，但自动选择 GPU/CPU，所有数据和模型均迁移到设备。
- **流程**：同 test04，增加设备选择和迁移

### 6. test06.py —— 使用Adam优化器
- **内容**：与 test04 类似，但使用 Adam 优化器自动管理参数更新。
- **流程**：
  1. nn.Sequential 定义网络
  2. Adam 优化器
  3. 前向传播、损失、反向传播
  4. optimizer.step() 自动更新参数

### 7. test07.py —— 动态结构神经网络
- **内容**：自定义 nn.Module，forward 中动态决定中间层数量（0~3），参数共享，体现 PyTorch 计算图的动态性。
- **流程**：
  1. 定义 DynamicNet 类
  2. 随机生成输入/输出
  3. 每次 forward 随机决定中间层数
  4. SGD 优化器训练

### 8. test08.py —— 线性回归最小示例
- **内容**：用 nn.Module 定义线性回归，Adam 优化器，训练数据极简，演示拟合 y=2x+1。
- **流程**：
  1. 定义 LinearRegression 类
  2. 构造极简训练数据
  3. Adam 优化器训练
  4. 预测新数据

### 9. test09.py —— PyTorch自动求导基础
- **内容**：演示 PyTorch 自动求导机制，计算 y=2x+3 关于 x 的梯度。
- **流程**：
  1. 创建可求导张量 x
  2. 构造表达式 y
  3. backward() 求导
  4. 输出 x 的梯度

---

## 适用人群
- 深度学习/PyTorch 初学者
- 需要理解神经网络底层原理与 PyTorch 自动求导机制的开发者

## 运行说明
- 建议使用 Python 3.7+，安装 pytorch >= 1.7
- 若有 GPU，部分脚本可自动使用 CUDA
- 直接运行各脚本即可观察训练损失收敛过程

---

如需进一步学习建议：
- 逐步阅读每个脚本，理解其与前一个脚本的区别
- 修改网络结构、数据规模、优化器等，观察训练效果变化
